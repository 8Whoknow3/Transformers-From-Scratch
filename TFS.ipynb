{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b8b41a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe046120",
   "metadata": {},
   "source": [
    "### Scaled Dot‑Product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b814cdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scaled_Dot_Product(nn.Module):\n",
    "    def __init__(self) :\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self , Q, K, V, d_k, mask = None) :\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None and mask.dim() == 2:\n",
    "            mask = mask.unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        if mask is not None :\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf')) \n",
    "        \n",
    "        attn_weights  = torch.softmax(scores, -1)\n",
    "        finale_output = torch.matmul(attn_weights, V)\n",
    "        \n",
    "        return finale_output, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a709638",
   "metadata": {},
   "source": [
    "### Multi‑Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "777b2c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multi_head(nn.Module) :\n",
    "    def __init__(self, num_heads, d_model):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "        self.W_Q = nn.Linear(d_model, d_model)\n",
    "        self.W_K = nn.Linear(d_model, d_model)\n",
    "        self.W_V = nn.Linear(d_model, d_model)\n",
    "        self.W_O = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.spd = Scaled_Dot_Product()\n",
    "    \n",
    "    def forward(self, Q, K, V, mask) :\n",
    "        seq_len = Q.size(1)\n",
    "\n",
    "        Q_proj = self.W_Q(Q)\n",
    "        K_proj = self.W_K(K)\n",
    "        V_proj = self.W_V(V)\n",
    "\n",
    "        batch_size = Q.size(0)\n",
    "\n",
    "        Q_proj = Q_proj.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K_proj = K_proj.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V_proj = V_proj.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        attn_output, _ = self.spd(Q_proj, K_proj, V_proj, self.d_k, mask)\n",
    "        \n",
    "        output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        return self.W_O(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5b47cf",
   "metadata": {},
   "source": [
    "### Feed-Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8e5c7bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFFN(nn.Module):\n",
    "    def __init__(self, d_model, d_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ff(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e3e1c4",
   "metadata": {},
   "source": [
    "### ADD & Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "aba0a3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNorm(nn.Module) :\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, raw_input, x) :\n",
    "        return self.norm(raw_input + self.dropout(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57fdcc5",
   "metadata": {},
   "source": [
    "### Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4c5225af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4f050f",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bf7f7eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module) :\n",
    "    def __init__(self, num_heads, d_model, d_ff, dropout, max_len):\n",
    "        super().__init__()\n",
    "        self.multihead = Multi_head(num_heads, d_model)\n",
    "        self.adn1 = AddNorm(d_model)\n",
    "        self.adn2 = AddNorm(d_model)\n",
    "        self.ffn = PositionwiseFFN(d_model, d_ff, dropout)\n",
    "        self.pe = PositionalEncoding(d_model, max_len)\n",
    "\n",
    "    def forward(self, x, mask = None) :\n",
    "        positional_encoder = self.pe(x)\n",
    "        x = self.adn1(positional_encoder, self.multihead(positional_encoder, positional_encoder, positional_encoder, mask))\n",
    "        x = self.adn2(x, self.ffn(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e1eb86",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d017e8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module) :\n",
    "    def __init__(self, num_heads, d_model, d_ff, dropout, max_len):\n",
    "        super().__init__()\n",
    "        self.mask_multi_head = Multi_head(num_heads, d_model)\n",
    "        self.multi_head = Multi_head(num_heads, d_model)\n",
    "        self.adn1 = AddNorm(d_model)\n",
    "        self.adn2 = AddNorm(d_model)\n",
    "        self.adn3 = AddNorm(d_model)\n",
    "        self.ffn = PositionwiseFFN(d_model, d_ff, dropout)\n",
    "        self.pe = PositionalEncoding(d_model, max_len)\n",
    "\n",
    "    def forward(self, x, encoder_output, mask = None) :\n",
    "        positional_encoder = self.pe(x)\n",
    "        x = self.adn1(positional_encoder, self.mask_multi_head(positional_encoder, positional_encoder, positional_encoder, mask))\n",
    "        x = self.adn2(x, self.multi_head(encoder_output, encoder_output, x, None))\n",
    "        x = self.adn3(x, self.ffn(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b36a610",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bfd74443",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, embedding_matrix, num_heads, d_ff, dropout, max_len):\n",
    "        super().__init__()\n",
    "        vocab_size, d_model = embedding_matrix.size()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
    "\n",
    "        self.encoder = Encoder(num_heads, d_model, d_ff, dropout, max_len)\n",
    "        self.decoder = Decoder(num_heads, d_model, d_ff, dropout, max_len)\n",
    "\n",
    "        self.output_layer = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, src_input_ids, tgt_input_ids, src_mask=None, tgt_mask=None):\n",
    "        if tgt_mask is None:\n",
    "            def generate_square_subsequent_mask(seq_len):\n",
    "                return torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0).unsqueeze(1).to(torch.bool)\n",
    "            tgt_mask = generate_square_subsequent_mask(tgt_input_ids.size(1)).to(tgt_input_ids.device)\n",
    "\n",
    "        src_emb = self.embedding(src_input_ids)\n",
    "        src_emb = self.pos_encoding(src_emb)\n",
    "        enc_output = self.encoder(src_emb, src_mask)\n",
    "\n",
    "        tgt_emb = self.embedding(tgt_input_ids)\n",
    "        tgt_emb = self.pos_encoding(tgt_emb)\n",
    "        dec_output = self.decoder(tgt_emb, enc_output, tgt_mask)\n",
    "\n",
    "        logits = self.output_layer(dec_output)\n",
    "        return logits\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
